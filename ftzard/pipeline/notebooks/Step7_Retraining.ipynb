{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785e2c3e-d2e9-4c2d-b76d-857ddacb7d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.pixi/envs/default/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow as mf\n",
    "import shutil\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,       \n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import concatenate_datasets\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import numpy as np\n",
    "from trl import SFTTrainer\n",
    "from hydra import initialize, compose\n",
    "import optuna\n",
    "import ftzard.utils.mlflow as mf_utils\n",
    "import joblib\n",
    "import dagstermill as dgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18f65cf-ebd7-468e-af95-43793a3993ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 25 14:34:56 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 40%   52C    P8     1W / 260W |     18MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 41%   62C    P8    37W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5424330-8d09-47c0-8057-c2a5e37ab54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 2\n",
      "Device 0: NVIDIA GeForce RTX 2080 Ti\n",
      "Device 1: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_devices}\")\n",
    "\n",
    "    # Print information about each device\n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"Device {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a9bfa7-fb08-47cc-bc23-661d6ad0fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../..'\n",
    "config_path = f'../../config/'\n",
    "data_path1 = f\"{base_path}/data/tokenized_dataset.joblib\"\n",
    "data_path2 = f\"{base_path}/data/retraining_data.joblib\"\n",
    "config_name = 'config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597bdc40-63ab-45a7-999c-ddb1197aece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=config_path):\n",
    "    cfg = compose(config_name=config_name)\n",
    "    tracking_uri, experiment_name = cfg.MLFLOW_TRACKING_URI, cfg.MLFLOW_EXPERIMENT_NAME\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756e94dd-e7fd-45ce-bb1e-6837ffc579ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Data Path:  ../../data/tokenized_dataset.joblib\n",
      "Sampled Data Path:  ../../data/retraining_data.joblib\n",
      "Mlflow Experiment Name:  senetiment_analysis\n",
      "Mlflow Run Name:  retraining\n",
      "Model Name:  tiiuae/falcon-7b\n"
     ]
    }
   ],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = tracking_uri\n",
    "run_name = 'retraining'\n",
    "model_name = cfg['model_name']\n",
    "max_len = 1024\n",
    "print('Previous Data Path: ', data_path1)\n",
    "print('Sampled Data Path: ', data_path2)\n",
    "print('Mlflow Experiment Name: ', experiment_name)\n",
    "print('Mlflow Run Name: ', run_name)\n",
    "print('Model Name: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbe9a17-058d-4b42-adca-4e497dfb493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = joblib.load(data_path1)[\"datasets\"]\n",
    "sampled_dataset = joblib.load(data_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b783e1-1c58-4ab6-a56f-eb9a62e4ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Previous Training Data ------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 14400\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1120\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n",
      "--------------- Sampled Data ------------------\n",
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 86\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------- Previous Training Data ------------------\")\n",
    "print(datasets)\n",
    "print(\"--------------- Sampled Data ------------------\")\n",
    "print(sampled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d53ab75-4b41-48f1-9070-6916d991bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_data = concatenate_datasets([datasets[\"train\"], sampled_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb19b4c8-5b47-46aa-b18c-f03cda4a2967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 14486\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(retrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db92a5ca-8d18-44f8-a910-4dcae92a74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75de36a-aa05-4552-b482-940eb1ee9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fbb4269-eac0-4b87-852a-28f31cc0a2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|           | 0/2 [00:00<?, ?it/s]/app/.pixi/envs/default/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.09s/it]\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForSequenceClassification(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=4544, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd47d2be-866a-4fcb-a3cf-8b5607382427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForSequenceClassification(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=4544, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f9ed953-1ca8-4b47-8691-4e4eaaaa876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(model, config):\n",
    "    return get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78902d79-7af7-4058-80b7-1b7cd65bbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7361b3a3-db93-4947-b009-925221398382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-25 14:37:27,510] A new study created in memory with name: no-name-8200f7e1-0981-4024-bab3-14d02a9f3753\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722a556-638b-4a78-8188-054b8ef1a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided experiment name senetiment_analysis already exists, the run will be logged in this experiment.\n",
      "                                 \n",
      "Experiment Id:  1\n",
      "Run Id:  be663a108e5d4bcb9751953d29d0de37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.pixi/envs/default/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2024/06/25 14:39:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id be663a108e5d4bcb9751953d29d0de37: Failed to log run data: Exception: Changing param values is not allowed. Params were already logged='[{'key': 'logging_dir', 'old_value': 'sentiment_classification_run_1/runs/Jun18_19-01-28_933e8d6f554d', 'new_value': 'sentiment_classification_run_1/runs/Jun25_14-39-53_8e802e6f43fc'}]' for run ID='be663a108e5d4bcb9751953d29d0de37'.\n",
      "2024/06/25 14:39:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id be663a108e5d4bcb9751953d29d0de37: Failed to log run data: Exception: Changing param values is not allowed. Params were already logged='[{'key': 'learning_rate', 'old_value': '0.00018373234106961294', 'new_value': '0.0002561582931817167'}, {'key': 'weight_decay', 'old_value': '0.0011609033084593473', 'new_value': '0.0070471347745436255'}]' for run ID='be663a108e5d4bcb9751953d29d0de37'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='589' max='1811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 589/1811 21:06 < 43:56, 0.46 it/s, Epoch 0.32/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    mf.end_run()\n",
    "artifact_path = \"mlartifacts\"\n",
    "experiment_id = mf_utils.create_experiment(exp_name=experiment_name)\n",
    "print('Experiment Id: ', experiment_id)\n",
    "with mf.start_run(experiment_id=experiment_id,\n",
    "                    run_name=run_name):\n",
    "    for i in range(1, 5):\n",
    "        trial = study.ask()\n",
    "        nested_run_name = f\"trial_{i}\"\n",
    "        run_id = mf_utils.get_run_id_by_name(run_name=nested_run_name, \n",
    "                                             experiment_ids=[experiment_id],\n",
    "                                            nested = True)\n",
    "        print('Run Id: ', run_id)\n",
    "        if run_id:\n",
    "            mf.start_run(run_id=run_id, run_name=nested_run_name, experiment_id=experiment_id, nested=True)\n",
    "        else:\n",
    "            mf.start_run(run_name=nested_run_name, experiment_id=experiment_id, nested=True)\n",
    "\n",
    "        ## CHOOSE HYPERAPARAMTERS ####\n",
    "\n",
    "        rank = trial.suggest_categorical(\"rank\", [8,16,32,64])\n",
    "        lr = trial.suggest_float(\"lr\", 0.00006, 0.0004,)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [8,16,32,64])\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 0.0005, 0.02)\n",
    "        lora_dropout = trial.suggest_float(\"lora_dropout\", 0.03, 0.06)\n",
    "        \n",
    "        try:\n",
    "            mf.log_params(trial.params)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "        r = rank, # the dimension of the low-rank matrices\n",
    "        lora_alpha = rank//2, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "        target_modules = [\n",
    "            \"query_key_value\",\n",
    "            \"dense\",\n",
    "            \"dense_h_to_4h\",\n",
    "            \"dense_4h_to_h\",\n",
    "            # \"score\"\n",
    "        ],\n",
    "        lora_dropout = lora_dropout, # dropout probability of the LoRA layers\n",
    "        bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "        task_type = 'SEQ_CLS'\n",
    "        )\n",
    "\n",
    "        lora_model = get_lora_model(model=model, config=lora_config)\n",
    "        lora_model.config.use_cache = False\n",
    "        lora_model.config.pretraining_tp = 1\n",
    "        lora_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "        output_dir = f'sentiment_classification_run_{i}',\n",
    "        learning_rate = lr,\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        num_train_epochs = 1,\n",
    "        weight_decay = weight_decay,\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        load_best_model_at_end = True\n",
    "        )\n",
    "        \n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "        model = lora_model,\n",
    "        args = training_args,\n",
    "        train_dataset = retrain_data,\n",
    "        eval_dataset = sampled_dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = collate_fn,\n",
    "        compute_metrics = compute_metrics,\n",
    "        )\n",
    "\n",
    "        result = trainer.train()\n",
    "        eval_res = trainer.evaluate()\n",
    "        study.tell(trial, eval_res['eval_accuracy'])\n",
    "        trainer.save_model(f'{base_path}/data/artifacts/run_{i}')\n",
    "        mf.log_artifacts(local_dir=f'{base_path}/data/artifacts/run_{i}')\n",
    "        mf.end_run()\n",
    "        shutil.rmtree(f'sentiment_classification_run_{i}', ignore_errors=True)\n",
    "        del lora_model\n",
    "        break\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
